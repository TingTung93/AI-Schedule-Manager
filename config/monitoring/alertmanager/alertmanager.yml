# AlertManager Configuration
global:
  smtp_smarthost: '${SMTP_HOST}:587'
  smtp_from: '${SMTP_FROM}'
  smtp_auth_username: '${SMTP_USER}'
  smtp_auth_password: '${SMTP_PASSWORD}'
  smtp_require_tls: true

# Email templates
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route configuration
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'
  routes:
    # Critical alerts go to PagerDuty and Slack immediately
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 0s
      repeat_interval: 5m

    # Error alerts go to Slack and email
    - match:
        severity: error
      receiver: 'error-alerts'
      group_wait: 30s
      repeat_interval: 15m

    # Warning alerts go to Slack only
    - match:
        severity: warning
      receiver: 'warning-alerts'
      group_wait: 2m
      repeat_interval: 1h

    # Info alerts are logged only
    - match:
        severity: info
      receiver: 'info-alerts'
      group_wait: 5m
      repeat_interval: 6h

# Alert receivers
receivers:
  # Default webhook
  - name: 'web.hook'
    webhook_configs:
      - url: 'http://host.docker.internal:3001/api/alerts/webhook'
        send_resolved: true

  # Critical alerts - PagerDuty + Slack + Email
  - name: 'critical-alerts'
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_ROUTING_KEY}'
        description: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        details:
          severity: '{{ .GroupLabels.severity }}'
          service: '{{ .GroupLabels.service }}'
          instance: '{{ .GroupLabels.instance }}'
          description: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        links:
          - href: '${GRAFANA_URL}/dashboard/alerts'
            text: 'View Dashboard'
          - href: '{{ range .Alerts }}{{ .Annotations.runbook_url }}{{ end }}'
            text: 'View Runbook'

    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#alerts'
        color: 'danger'
        title: 'CRITICAL: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        text: |
          {{ range .Alerts }}
          *Service:* {{ .Labels.service }}
          *Instance:* {{ .Labels.instance }}
          *Description:* {{ .Annotations.description }}
          *Runbook:* {{ .Annotations.runbook_url }}
          {{ end }}

    email_configs:
      - to: '${ONCALL_EMAIL}'
        subject: 'CRITICAL ALERT: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        body: |
          Critical alert fired in AI Schedule Manager.

          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Service: {{ .Labels.service }}
          Instance: {{ .Labels.instance }}
          Severity: {{ .Labels.severity }}

          Description: {{ .Annotations.description }}

          Dashboard: ${GRAFANA_URL}/dashboard/alerts
          Runbook: {{ .Annotations.runbook_url }}

          Started: {{ .StartsAt }}
          {{ end }}

  # Error alerts - Slack + Email
  - name: 'error-alerts'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#errors'
        color: 'warning'
        title: 'ERROR: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        text: |
          {{ range .Alerts }}
          *Service:* {{ .Labels.service }}
          *Instance:* {{ .Labels.instance }}
          *Description:* {{ .Annotations.description }}
          {{ end }}

    email_configs:
      - to: '${TEAM_EMAIL}'
        subject: 'Error Alert: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        body: |
          Error detected in AI Schedule Manager.

          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Service: {{ .Labels.service }}
          Instance: {{ .Labels.instance }}
          Description: {{ .Annotations.description }}
          Started: {{ .StartsAt }}
          {{ end }}

  # Warning alerts - Slack only
  - name: 'warning-alerts'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#monitoring'
        color: 'warning'
        title: 'WARNING: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        text: |
          {{ range .Alerts }}
          *Service:* {{ .Labels.service }}
          *Instance:* {{ .Labels.instance }}
          *Description:* {{ .Annotations.description }}
          {{ end }}

  # Info alerts - Webhook only
  - name: 'info-alerts'
    webhook_configs:
      - url: 'http://host.docker.internal:3001/api/alerts/info'
        send_resolved: true

# Inhibition rules
inhibit_rules:
  # Inhibit warning and info alerts if critical alert is firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'service', 'instance']

  - source_match:
      severity: 'critical'
    target_match:
      severity: 'info'
    equal: ['alertname', 'service', 'instance']

  # Inhibit info alerts if error alert is firing
  - source_match:
      severity: 'error'
    target_match:
      severity: 'info'
    equal: ['alertname', 'service', 'instance']
# Fluentd Configuration for AI Schedule Manager
# Collects logs from various sources and forwards to Elasticsearch

# Input Sources
<source>
  @type tail
  @id backend_logs
  path /var/log/app/backend/*.log
  pos_file /var/log/fluentd-backend.log.pos
  tag ai-schedule.backend
  format json
  time_key timestamp
  time_format %Y-%m-%dT%H:%M:%S.%L%z
  read_from_head true
  refresh_interval 10
</source>

<source>
  @type tail
  @id frontend_logs
  path /var/log/app/frontend/*.log
  pos_file /var/log/fluentd-frontend.log.pos
  tag ai-schedule.frontend
  format json
  time_key timestamp
  time_format %Y-%m-%dT%H:%M:%S.%L%z
  read_from_head true
  refresh_interval 10
</source>

<source>
  @type tail
  @id nginx_access_logs
  path /var/log/nginx/access.log
  pos_file /var/log/fluentd-nginx-access.log.pos
  tag ai-schedule.nginx.access
  format nginx
  read_from_head true
</source>

<source>
  @type tail
  @id nginx_error_logs
  path /var/log/nginx/error.log
  pos_file /var/log/fluentd-nginx-error.log.pos
  tag ai-schedule.nginx.error
  format /^(?<time>\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}) \[(?<log_level>\w+)\] (?<pid>\d+)#(?<tid>\d+): (?<message>.*)/
  read_from_head true
</source>

# Kubernetes logs
<source>
  @type tail
  @id kubernetes_logs
  path /var/log/containers/*ai-schedule*.log
  pos_file /var/log/fluentd-kubernetes.log.pos
  tag kubernetes.*
  format json
  read_from_head true
  <parse>
    @type multi_format
    <pattern>
      format json
      time_key time
      time_format %Y-%m-%dT%H:%M:%S.%NZ
    </pattern>
    <pattern>
      format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
      time_format %Y-%m-%dT%H:%M:%S.%N%:z
    </pattern>
  </parse>
</source>

# System logs
<source>
  @type systemd
  @id systemd_logs
  matches [{"_SYSTEMD_UNIT": "docker.service"}, {"_SYSTEMD_UNIT": "kubelet.service"}]
  tag systemd
  read_from_head true
  strip_underscores true
</source>

# Filters and Processing
<filter ai-schedule.**>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    environment "#{ENV['ENVIRONMENT'] || 'production'}"
    service_name ${tag_parts[1]}
    log_type application
  </record>
</filter>

<filter kubernetes.**>
  @type kubernetes_metadata
  @id kubernetes_metadata
  kubernetes_url "#{ENV['KUBERNETES_SERVICE_HOST']}:#{ENV['KUBERNETES_SERVICE_PORT_HTTPS']}"
  verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL'] || true}"
  ca_file "#{ENV['KUBERNETES_CA_FILE']}"
  skip_labels false
  skip_container_metadata false
  skip_master_url false
  skip_namespace_metadata false
</filter>

<filter kubernetes.**>
  @type record_transformer
  <record>
    service_name ${record["kubernetes"]["labels"]["app"] || "unknown"}
    environment ${record["kubernetes"]["namespace_name"] || "unknown"}
    log_type kubernetes
  </record>
</filter>

# Security log filtering
<filter ai-schedule.**>
  @type grep
  <regexp>
    key log_level
    pattern ^(ERROR|WARN|INFO)$
  </regexp>
</filter>

# Rate limiting for high-volume logs
<filter ai-schedule.backend>
  @type throttle
  group_key service_name
  group_bucket_period_s 60
  group_bucket_limit 1000
</filter>

# Error log enrichment
<filter ai-schedule.**>
  @type record_transformer
  enable_ruby
  <record>
    severity ${record["log_level"] == "ERROR" ? "high" : record["log_level"] == "WARN" ? "medium" : "low"}
    alert_required ${record["log_level"] == "ERROR" ? "true" : "false"}
  </record>
</filter>

# Output Configurations

# Main Elasticsearch output
<match ai-schedule.**>
  @type elasticsearch
  @id elasticsearch_main
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
  user "#{ENV['ELASTICSEARCH_USER'] || ''}"
  password "#{ENV['ELASTICSEARCH_PASSWORD'] || ''}"
  scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"

  index_name ai-schedule-logs
  type_name _doc

  # Index template
  template_name ai-schedule-template
  template_file /fluentd/etc/elasticsearch-template.json
  template_overwrite true

  # Buffer configuration
  <buffer>
    @type file
    path /var/log/fluentd-buffers/elasticsearch
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 2
    flush_interval 5s
    retry_forever
    retry_max_interval 30
    chunk_limit_size 2M
    queue_limit_length 8
    overflow_action block
  </buffer>

  # Performance tuning
  bulk_message_request_threshold 1024
  bulk_message_request_max_timeout 5
  request_timeout 120s
  reload_connections false
  reconnect_on_error true
  reload_on_failure true

  # Error handling
  log_es_400_reason true
  logstash_format true
  logstash_prefix ai-schedule
  logstash_dateformat %Y.%m.%d

  # Health check
  verify_es_version_at_startup true
  default_elasticsearch_version 7
</match>

# Kubernetes logs to separate index
<match kubernetes.**>
  @type elasticsearch
  @id elasticsearch_kubernetes
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
  user "#{ENV['ELASTICSEARCH_USER'] || ''}"
  password "#{ENV['ELASTICSEARCH_PASSWORD'] || ''}"
  scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"

  index_name kubernetes-logs
  type_name _doc

  <buffer>
    @type file
    path /var/log/fluentd-buffers/kubernetes
    flush_mode interval
    flush_interval 10s
    chunk_limit_size 2M
    queue_limit_length 8
  </buffer>

  logstash_format true
  logstash_prefix kubernetes
  logstash_dateformat %Y.%m.%d
</match>

# System logs
<match systemd>
  @type elasticsearch
  @id elasticsearch_systemd
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
  user "#{ENV['ELASTICSEARCH_USER'] || ''}"
  password "#{ENV['ELASTICSEARCH_PASSWORD'] || ''}"
  scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"

  index_name systemd-logs
  type_name _doc

  <buffer>
    @type file
    path /var/log/fluentd-buffers/systemd
    flush_mode interval
    flush_interval 30s
    chunk_limit_size 1M
  </buffer>

  logstash_format true
  logstash_prefix systemd
  logstash_dateformat %Y.%m.%d
</match>

# Dead letter queue for failed messages
<match **>
  @type file
  @id file_dead_letter
  path /var/log/fluentd-dead-letter/dead-letter
  append true
  time_slice_format %Y%m%d
  time_slice_wait 10m
  time_format %Y%m%dT%H%M%S%z
  compress gzip
</match>

# Monitoring and Health Check
<source>
  @type monitor_agent
  bind 0.0.0.0
  port 24220
  tag fluentd.monitor.metrics
</source>

# Forward to monitoring if needed
<match fluentd.monitor.metrics>
  @type stdout
</match>

# Log rotation and cleanup
<system>
  log_level "#{ENV['FLUENTD_LOG_LEVEL'] || 'info'}"
  suppress_repeated_stacktrace true
  emit_error_log_interval 30s
  suppress_config_dump true
  without_source true

  <log>
    format json
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </log>
</system>
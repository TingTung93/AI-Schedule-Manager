# Fluentd Configuration for Log Aggregation
# Collects logs from multiple sources and forwards to Elasticsearch

# HTTP input for application logs
<source>
  @type http
  port 9880
  bind 0.0.0.0
  body_size_limit 32m
  keepalive_timeout 10s

  <parse>
    @type json
    time_key @timestamp
    time_format %Y-%m-%dT%H:%M:%S.%L%z
  </parse>
</source>

# Tail application log files
<source>
  @type tail
  path /app/logs/application-*.log
  pos_file /var/log/fluentd/application.log.pos
  tag application.logs

  <parse>
    @type json
    time_key @timestamp
    time_format %Y-%m-%dT%H:%M:%S.%L%z
  </parse>
</source>

# Tail error log files
<source>
  @type tail
  path /app/logs/error-*.log
  pos_file /var/log/fluentd/error.log.pos
  tag application.errors

  <parse>
    @type json
    time_key @timestamp
    time_format %Y-%m-%dT%H:%M:%S.%L%z
  </parse>
</source>

# Docker container logs
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# System logs
<source>
  @type systemd
  path /var/log/journal
  matches [{ "_SYSTEMD_UNIT": "ai-schedule-manager.service" }]
  tag system.ai-schedule-manager

  <storage>
    @type local
    persistent true
    path /var/log/fluentd/systemd.pos
  </storage>
</source>

# Nginx access logs
<source>
  @type tail
  path /var/log/nginx/access.log
  pos_file /var/log/fluentd/nginx-access.log.pos
  tag nginx.access

  <parse>
    @type nginx
    keep_time_key true
  </parse>
</source>

# Nginx error logs
<source>
  @type tail
  path /var/log/nginx/error.log
  pos_file /var/log/fluentd/nginx-error.log.pos
  tag nginx.error

  <parse>
    @type multiline
    format_firstline /\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}/
    format1 /^(?<time>\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}) \[(?<log_level>\w+)\] (?<pid>\d+).(?<tid>\d+): (?<message>.*)/
  </parse>
</source>

# Filter to add environment and service metadata
<filter **>
  @type record_transformer

  <record>
    environment "#{ENV['ENVIRONMENT'] || 'production'}"
    service "ai-schedule-manager"
    hostname "#{Socket.gethostname}"
    fluentd_worker "#{worker_id}"
  </record>
</filter>

# Parse and enrich application logs
<filter application.**>
  @type parser
  key_name message
  reserve_data true
  inject_key_prefix parsed_

  <parse>
    @type json
  </parse>
</filter>

# GeoIP enrichment for nginx logs
<filter nginx.access>
  @type geoip
  geoip_lookup_keys remote_addr

  <record>
    location_country ${country_name["remote_addr"]}
    location_city ${city_name["remote_addr"]}
    location_latitude ${latitude["remote_addr"]}
    location_longitude ${longitude["remote_addr"]}
  </record>
</filter>

# Prometheus metrics output
<match **>
  @type prometheus

  <metric>
    name fluentd_input_status_num_records_total
    type counter
    desc The total number of incoming records
    <labels>
      tag ${tag}
      hostname ${hostname}
    </labels>
  </metric>
</match>

# Buffer configuration for Elasticsearch output
<match {application.**,system.**,nginx.**}>
  @type elasticsearch
  host "#{ENV['ELASTICSEARCH_HOST'] || 'localhost'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
  user "#{ENV['ELASTICSEARCH_USERNAME']}"
  password "#{ENV['ELASTICSEARCH_PASSWORD']}"

  scheme https
  ssl_verify false

  index_name ai-schedule-manager-logs
  type_name _doc

  include_timestamp true
  logstash_format true
  logstash_prefix ai-schedule-manager

  <buffer>
    @type file
    path /var/log/fluentd/buffer/elasticsearch
    flush_mode interval
    flush_interval 5s
    flush_thread_count 2
    retry_type exponential_backoff
    retry_wait 1s
    retry_max_interval 60s
    retry_timeout 60m
    queue_limit_length 1024
    chunk_limit_size 8m
    total_limit_size 1g
    overflow_action drop_oldest_chunk
  </buffer>
</match>

# Backup output to file in case Elasticsearch is down
<match **>
  @type file
  path /var/log/fluentd/backup/backup.%Y%m%d.log
  append true
  time_slice_format %Y%m%d
  time_slice_wait 1m
  compress gzip

  <format>
    @type json
  </format>

  <buffer time>
    timekey 1h
    timekey_wait 10m
    timekey_use_utc true
  </buffer>
</match>
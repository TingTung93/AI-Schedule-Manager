# Docker Network Analysis Report
**Generated:** 2025-11-20T16:59:30Z
**Branch:** fix/api-routing-and-response-handling

## Executive Summary

### Critical Findings
1. **BACKEND IS HANGING** - The backend container appears healthy when tested internally but hangs indefinitely (3+ minutes timeout) when accessed from other containers
2. **Network connectivity is functional** - All containers are on the correct network with proper DNS resolution
3. **Frontend health checks are failing** - Due to backend unresponsiveness, not network issues
4. **This is a recurring application-level bug** - Not a Docker networking or configuration problem

---

## Network Configuration Analysis

### 1. Network Topology ‚úÖ CORRECT

**Network:** `ai-schedule-manager_app-network`
- **Type:** Bridge driver
- **Subnet:** 172.18.0.0/16
- **Gateway:** 172.18.0.1

**Container IP Addresses:**
```
ai-schedule-redis      172.18.0.2/16 (HEALTHY)
ai-schedule-db         172.18.0.3/16 (HEALTHY)
ai-schedule-backend    172.18.0.4/16 (UNHEALTHY)
ai-schedule-frontend   172.18.0.5/16 (UNHEALTHY)
```

**Verdict:** All containers are properly connected to the same bridge network with correct subnet allocation.

---

### 2. Service Dependencies ‚úÖ CORRECT

**Dependency Chain:**
```
frontend ‚Üí backend ‚Üí [postgres, redis]
                ‚Üì
           Both HEALTHY
```

**docker-compose.yml analysis:**
- Backend correctly depends on postgres and redis with `service_healthy` condition
- Frontend correctly depends on backend
- Postgres and Redis are HEALTHY and responding correctly

**Verdict:** Dependency configuration is correct. Database and cache layers are operational.

---

### 3. DNS Resolution ‚úÖ WORKING

**nginx.conf DNS Configuration:**
```nginx
resolver 127.0.0.11 valid=30s;  # Docker internal DNS
set $backend http://ai-schedule-backend:8000;
proxy_pass $backend;
```

**Test Results:**
- ‚úÖ DNS resolves `ai-schedule-backend` to `172.18.0.4`
- ‚úÖ Frontend can connect to backend IP address
- ‚úÖ Backend container name resolution working

**Verdict:** DNS resolution is functioning correctly. Nginx is using Docker's internal DNS resolver properly.

---

### 4. Health Check Analysis üî¥ CRITICAL ISSUE

#### Backend Health Check
**Configuration:**
```dockerfile
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1
```

**Test Results:**

**Internal Test (localhost):**
```bash
$ docker exec ai-schedule-backend curl -f http://localhost:8000/health
# HUNG FOR 3+ MINUTES (timeout: 180 seconds)
# Eventually returned: Backend health check failed
```

**BUT PARADOXICALLY:**
```bash
$ docker inspect ai-schedule-backend --format='{{.State.Health.Status}}'
healthy

$ curl -f http://localhost:8000/health
{"status":"healthy","timestamp":"2025-11-20T16:59:27.140923"}
```

**Cross-Container Test:**
```bash
$ docker exec ai-schedule-frontend wget -O- http://ai-schedule-backend:8000/health
Connecting to ai-schedule-backend:8000 (172.18.0.4:8000)
wget: error getting response: Connection reset by peer
```

**Critical Finding:** The backend responds to the Docker health check but hangs on actual API requests.

#### Frontend Health Check
**Configuration:**
```dockerfile
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD wget --no-verbose --tries=1 --spider http://localhost:80/health || exit 1
```

**nginx.conf Health Endpoint:**
```nginx
location /health {
    access_log off;
    return 200 "healthy\n";
    add_header Content-Type text/plain;
}
```

**Frontend Logs Analysis:**
```
172.18.0.1 - - [20/Nov/2025:16:38:21 +0000] "GET /api/auth/csrf-token HTTP/1.1" 200 60
172.18.0.1 - - [20/Nov/2025:16:38:31 +0000] "GET /api/analytics/overview HTTP/1.1" 499 0
172.18.0.1 - - [20/Nov/2025:16:38:31 +0000] "GET /api/employees?page=1&size=10 HTTP/1.1" 499 0
```

**Status Code 499:** Client closed connection (timeout) - nginx's way of saying the upstream (backend) never responded.

**Verdict:** Frontend health checks fail because the backend is unresponsive to proxied requests.

---

### 5. Port Mapping Analysis ‚úÖ CORRECT

**Port Mappings:**
```
Frontend:  0.0.0.0:80    ‚Üí 80   (HTTP)
Backend:   0.0.0.0:8000  ‚Üí 8000 (FastAPI)
Postgres:  0.0.0.0:5432  ‚Üí 5432 (PostgreSQL) - WARNING: Should not be exposed in production
Redis:     0.0.0.0:6379  ‚Üí 6379 (Redis) - WARNING: Should not be exposed in production
```

**Verdict:** Port mappings are correct. Security note: Database ports should not be exposed in production.

---

### 6. Proxy Configuration Analysis ‚úÖ CORRECT

**nginx.conf Proxy Settings:**
```nginx
location /api {
    set $backend http://ai-schedule-backend:8000;
    proxy_pass $backend;
    proxy_http_version 1.1;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
    proxy_read_timeout 300;      # 5 minute timeout
    proxy_connect_timeout 300;   # 5 minute timeout
}
```

**Analysis:**
- ‚úÖ Correct backend container name: `ai-schedule-backend`
- ‚úÖ Correct port: `8000`
- ‚úÖ Proper headers forwarding
- ‚úÖ Generous timeouts (300s) - rules out timeout issues
- ‚úÖ Using variable for DNS re-resolution

**Verdict:** Nginx proxy configuration is optimal and correct.

---

### 7. Container Startup Analysis

**Backend Startup Script (`/app/scripts/startup.sh`):**
```bash
1. Wait for PostgreSQL (pg_isready)
2. Run Alembic migrations (alembic upgrade head)
3. Seed database (python scripts/seed_data.py)
4. Start uvicorn (--host 0.0.0.0 --port 8000 --reload)
```

**Observations:**
- ‚úÖ Proper database wait logic
- ‚úÖ Migrations run before app start
- ‚úÖ Uvicorn configured to listen on all interfaces
- ‚ö†Ô∏è `--reload` flag enabled (development mode)

**Verdict:** Startup sequence is correct but runs in development mode.

---

## Root Cause Analysis

### The Backend Hang Problem

**Symptoms:**
1. Backend container status: UNHEALTHY (despite health checks passing)
2. Backend responds to health checks from Docker but not from nginx/frontend
3. All API requests hang for 3+ minutes before timing out
4. Connection reset errors from inter-container communication

**Evidence:**
```
Internal health check:  HANGS (3+ min) ‚Üí fails
Docker health check:    PASSES ‚Üí reports healthy
Cross-container:        Connection reset by peer
Frontend logs:          499 (client closed, upstream timeout)
```

**This is NOT a networking issue. This is an application-level problem.**

### Possible Causes

1. **Backend Event Loop Blocking**
   - FastAPI/Uvicorn worker is blocking on some synchronous operation
   - Health endpoint responds because it's simple, but complex endpoints hang
   - Async operations may be blocking the event loop

2. **Database Connection Pool Exhaustion**
   - Backend may be holding database connections
   - New requests wait for available connections
   - Health check succeeds because it doesn't use DB

3. **Middleware/Authentication Hang**
   - Authentication middleware may be hanging on token validation
   - CSRF validation may be blocking
   - Session management may be waiting indefinitely

4. **Race Condition in Startup**
   - Migrations or seeding may have left database in inconsistent state
   - Backend starts before database is truly ready
   - Connection pool initialization issues

5. **Memory/Resource Exhaustion**
   - Container may be running out of memory
   - Python GC may be causing pauses
   - Log file growth causing disk I/O blocking

---

## Recommendations

### Immediate Actions (Priority 1)

1. **Check Backend Application Logs**
   ```bash
   docker logs ai-schedule-backend --tail 100 -f
   ```
   Look for: error messages, stack traces, hung operations

2. **Check Python Process State**
   ```bash
   docker exec ai-schedule-backend python -c "import os; print(os.getpid())"
   docker top ai-schedule-backend
   ```

3. **Test Backend Directly (Bypass Nginx)**
   ```bash
   curl -v http://localhost:8000/health
   curl -v http://localhost:8000/api/auth/csrf-token
   ```

4. **Check Database Connection State**
   ```bash
   docker exec ai-schedule-db psql -U postgres -d schedule_manager -c "SELECT count(*) FROM pg_stat_activity;"
   ```

5. **Restart Backend Container with Fresh State**
   ```bash
   docker-compose down backend
   docker-compose up -d backend
   docker logs -f ai-schedule-backend
   ```

### Code Investigation (Priority 2)

**Files to Investigate:**
1. `/home/peter/AI-Schedule-Manager/backend/src/main.py` - Application initialization
2. `/home/peter/AI-Schedule-Manager/backend/src/middleware/` - Authentication/CSRF middleware
3. `/home/peter/AI-Schedule-Manager/backend/src/db/database.py` - Database connection pool
4. `/home/peter/AI-Schedule-Manager/backend/src/api/auth.py` - Authentication endpoints

**Look for:**
- Blocking synchronous calls in async functions
- Database queries without timeouts
- Infinite loops or retries
- Session/connection cleanup issues

### Configuration Changes (Priority 3)

1. **Add Request Timeout to Backend**
   ```python
   # In main.py or middleware
   @app.middleware("http")
   async def timeout_middleware(request: Request, call_next):
       try:
           return await asyncio.wait_for(call_next(request), timeout=30.0)
       except asyncio.TimeoutError:
           return JSONResponse({"error": "Request timeout"}, status_code=504)
   ```

2. **Add Database Connection Pool Limits**
   ```python
   # In database.py
   engine = create_async_engine(
       DATABASE_URL,
       pool_size=5,
       max_overflow=10,
       pool_timeout=30,
       pool_recycle=3600
   )
   ```

3. **Improve Health Check**
   ```python
   @app.get("/health")
   async def health_check(db: AsyncSession = Depends(get_db)):
       # Test database connectivity
       try:
           await db.execute(text("SELECT 1"))
           return {"status": "healthy", "database": "connected"}
       except Exception as e:
           return {"status": "unhealthy", "error": str(e)}
   ```

4. **Add Monitoring Endpoint**
   ```python
   @app.get("/metrics")
   async def metrics():
       return {
           "active_connections": db_pool.active_connections,
           "pool_size": db_pool.pool_size,
           "overflow": db_pool.overflow,
           "timestamp": datetime.utcnow()
       }
   ```

---

## Network Configuration Score

| Component | Status | Score | Notes |
|-----------|--------|-------|-------|
| Network Topology | ‚úÖ | 10/10 | Perfect bridge network setup |
| DNS Resolution | ‚úÖ | 10/10 | Working correctly |
| Service Dependencies | ‚úÖ | 10/10 | Proper dependency chain |
| Port Mappings | ‚úÖ | 9/10 | Correct but DB ports exposed |
| Nginx Proxy Config | ‚úÖ | 10/10 | Optimal configuration |
| Health Checks | üî¥ | 3/10 | Backend hanging on requests |
| Overall Network | ‚úÖ | 9/10 | Network is fine, app is not |

---

## Conclusion

**The Docker networking configuration is CORRECT and FUNCTIONAL.**

The UNHEALTHY status of both containers is caused by **backend application hangs**, not network or configuration issues. The backend is experiencing a recurring bug where it:

1. Responds to simple health checks
2. Hangs indefinitely on actual API requests
3. Causes connection resets and timeouts
4. Blocks all frontend functionality

This appears to be related to:
- Event loop blocking
- Database connection issues
- Middleware/authentication problems
- Or a combination of these factors

**Next Steps:**
1. Investigate backend application logs
2. Review async/await patterns in the codebase
3. Check database connection pool configuration
4. Add request-level timeouts
5. Implement comprehensive monitoring

The networking layer is solid - focus all efforts on the application layer.

---

## Related Issues

Based on git log:
- `d1a2deb` - Fixed 401 redirect issue (token clearing)
- `85790e5` - Fixed backend hang and API errors (CRITICAL FIX) - **THIS IS A RECURRING ISSUE**
- `0b7d3dc` - Added backend monitoring and notifications fix
- `6cf933d` - Resolved API routing and authentication issues

**Pattern:** This is a known recurring backend hang issue that has been "fixed" multiple times but keeps returning. The root cause has not been identified.
